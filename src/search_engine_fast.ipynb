{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/aaditya11/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def docPreProcessing(filepath):\n",
    "#     stopwords_dict = Counter(stopwords.words('english'))\n",
    "\n",
    "#     file = open(filepath, 'r', errors='replace')\n",
    "#     lines = file.readlines()\n",
    "    \n",
    "#     # print(filepath)\n",
    "\n",
    "#     stopwords_dict = Counter(stopwords.words('english'))\n",
    "    \n",
    "    \n",
    "#     doctext = \"\"\n",
    "#     for line in lines:\n",
    "#         line = line.translate(str.maketrans('', '', string.punctuation)).strip().lower()\n",
    "#         line = ' '.join([word for word in line.split() if word not in stopwords_dict])\n",
    "#         doctext += line\n",
    "        \n",
    "#     doctextList = doctext.split()\n",
    "#     return doctextList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docPreProcessing(filepath):\n",
    "    stopwords_dict = Counter(stopwords.words('english'))\n",
    "\n",
    "\n",
    "    docTextList = []\n",
    "    with open(filepath, 'r', errors='replace') as file:\n",
    "        for line in file:\n",
    "            for word in line.split():\n",
    "                word = word.translate(str.maketrans('', '', string.punctuation)).strip().lower()\n",
    "                if word not in stopwords_dict and word != '':\n",
    "                    docTextList.append(word)\n",
    "\n",
    "    return docTextList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ettealcsnorg ettealcsnorg\n",
      "eric eric\n",
      "h h\n",
      "taylor taylor\n",
      "subject subject\n",
      "gravity graviti\n",
      "waves wave\n",
      "predicting predict\n",
      "gravity graviti\n",
      "wave wave\n",
      "quantization quantiz\n",
      "cosmic cosmic\n",
      "noise nois\n",
      "article articl\n",
      "c4kvjf4qowellsfcaus c4kvjf4qowellsfcau\n",
      "metareswellsfcaus metareswellsfcau\n",
      "tom tom\n",
      "van van\n",
      "flandern flandern\n",
      "writes write\n",
      "crb7qkelvinseasvirginiaedu crb7qkelvinseasvirginiaedu\n",
      "cameron cameron\n",
      "randale randal\n",
      "bass bass\n",
      "writes write\n",
      "brucescottlaunchpaduncedu brucescottlaunchpaduncedu\n",
      "bruce bruce\n",
      "scott scott\n",
      "writes write\n",
      "existence exist\n",
      "undefined undefin\n",
      "unless unless\n",
      "synonymous synonym\n",
      "observable observ\n",
      "physics physic\n",
      "crb crb\n",
      "dong dong\n",
      "dong dong\n",
      "dong dong\n",
      "hear hear\n",
      "deathknell deathknel\n",
      "string string\n",
      "theory theori\n",
      "agree agre\n",
      "add add\n",
      "dark dark\n",
      "matter matter\n",
      "quarks quark\n",
      "lot lot\n",
      "unobservable unobserv\n",
      "purely pure\n",
      "theoretical theoret\n",
      "constructs construct\n",
      "physics physic\n",
      "list list\n",
      "including includ\n",
      "omnipresent omnipres\n",
      "black black\n",
      "holes hole\n",
      "bruce bruce\n",
      "argue argu\n",
      "existence exist\n",
      "inferred infer\n",
      "theory theori\n",
      "alone alon\n",
      "original origin\n",
      "criticism critic\n",
      "said said\n",
      "curvature curvatur\n",
      "exist exist\n",
      "relative rel\n",
      "something someth\n",
      "noncurved noncurv\n",
      "bruce bruce\n",
      "replied repli\n",
      "existence exist\n",
      "undefined undefin\n",
      "unless unless\n",
      "synonymous synonym\n",
      "observable observ\n",
      "physics physic\n",
      "cannot cannot\n",
      "observe observ\n",
      "four four\n",
      "dimensions dimens\n",
      "know know\n",
      "moment moment\n",
      "dont dont\n",
      "see see\n",
      "way way\n",
      "defend defend\n",
      "statement statement\n",
      "existence exist\n",
      "unobservable unobserv\n",
      "phenomena phenomena\n",
      "simultaneously simultan\n",
      "tom tom\n",
      "hold hold\n",
      "space space\n",
      "cannot cannot\n",
      "curved curv\n",
      "simple simpl\n",
      "reason reason\n",
      "properties properti\n",
      "properties properti\n",
      "speak speak\n",
      "dealing deal\n",
      "matter matter\n",
      "filling fill\n",
      "space space\n",
      "say say\n",
      "presence presenc\n",
      "large larg\n",
      "bodies bodi\n",
      "space space\n",
      "becomes becom\n",
      "curved curv\n",
      "equivalent equival\n",
      "stating state\n",
      "something someth\n",
      "act act\n",
      "upon upon\n",
      "nothing noth\n",
      "one one\n",
      "refuse refus\n",
      "subscribe subscrib\n",
      "view view\n",
      "nikola nikola\n",
      "tesla tesla\n",
      "et et\n",
      "tesla tesla\n",
      "100 100\n",
      "years year\n",
      "ahead ahead\n",
      "time time\n",
      "perhaps perhap\n",
      "time time\n",
      "comes come\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "wordlist = docPreProcessing(r'../testset/sci.space/59497')\n",
    "\n",
    "for word in wordlist:\n",
    "    print(word + \" \" + ps.stem(word))\n",
    "\n",
    "# docPreProcessing(r'../testset/sci.space/59497')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(wordlist):\n",
    "    # global tfcount\n",
    "    # tfcount += 1\n",
    "    wordmap = {}\n",
    "    \n",
    "    for word in wordlist:\n",
    "        if word in wordmap:\n",
    "            wordmap[word] += 1\n",
    "        else:\n",
    "            wordmap[word] = 1\n",
    "            \n",
    "    \n",
    "    numWords = len(wordlist)\n",
    "    \n",
    "    for word in wordmap:\n",
    "        wordmap[word] = wordmap[word] / numWords\n",
    "    \n",
    "    return wordmap\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tfidfMapBuilder(doclist):\n",
    "#     # termset = set()\n",
    "\n",
    "#     # for doc in doclist:\n",
    "#         # termset.update(set(docPreProcessing(doc)))\n",
    "\n",
    "#     # df_columns = doclist.copy()\n",
    "#     df_columns = []\n",
    "#     df_columns.append('df')\n",
    "#     df_columns.extend(doclist)\n",
    "#     # print(df_columns)\n",
    "\n",
    "#     df = pd.DataFrame(data=0, index = [], columns=df_columns)\n",
    "\n",
    "#     for doc in doclist:\n",
    "#         wordmap = tf(docPreProcessing(doc))\n",
    "#         for term in wordmap:\n",
    "#             if term not in df.index:\n",
    "#                 new_row = pd.DataFrame(data=0, index=[term], columns=df_columns)\n",
    "#                 df = pd.concat([df, new_row])\n",
    "#             df.loc[term, doc] = wordmap[term]\n",
    "#             df.loc[term, 'df'] += 1\n",
    "\n",
    "#     # print(df)\n",
    "\n",
    "#     # N = len(doclist)\n",
    "#     # df['df'] = df['df'].apply(lambda x: math.log(N/x))\n",
    "\n",
    "#     return df\n",
    "\n",
    "#     # print(len(termset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tfidfMapBuilder(doclist):\n",
    "    termset = set()\n",
    "\n",
    "    for doc in doclist:\n",
    "        termset.update(set(tf(docPreProcessing(doc)).keys()))\n",
    "\n",
    "    df_columns = doclist.copy()\n",
    "    df_columns.append('df')\n",
    "    print(df_columns)\n",
    "\n",
    "    df = pd.DataFrame(0, index=list(termset), columns=df_columns)\n",
    "\n",
    "    for doc in doclist:\n",
    "        wordmap = tf(docPreProcessing(doc))\n",
    "        for term in wordmap:\n",
    "            df.at[term, doc] = wordmap[term]\n",
    "            df.at[term, 'df'] += 1\n",
    "\n",
    "    # print(df)\n",
    "\n",
    "    N = len(doclist)\n",
    "    # df['df'] = df['df'].apply(lambda x: math.log(N/x))\n",
    "\n",
    "    return df\n",
    "\n",
    "    # print(len(termset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapToMatrix(df, N):\n",
    "    df['df'] = df['df'].apply(lambda x: math.log(N/x))\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        term_df = row['df']\n",
    "        row = row.apply(lambda x: x * term_df)\n",
    "    \n",
    "    df = df.drop(columns=['df'], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarityScore(q, D):\n",
    "    q_mag = np.sqrt(q.dot(q))\n",
    "    \n",
    "    qT = np.reshape(q, (1, q.shape[0]))\n",
    "    qTD = np.matmul(qT, D).reshape((D.shape[1], ))\n",
    "    \n",
    "    D_mags = np.sqrt(np.sum(D*D, axis=0))\n",
    "    divisors = q_mag * D_mags\n",
    "    cos_thetas = np.divide(qTD, divisors)\n",
    "    \n",
    "    cos_thetas = np.clip(cos_thetas, -1, 1)\n",
    "    \n",
    "    scores = np.arccos(cos_thetas)\n",
    "    \n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileCollector(path):\n",
    "    filelist = []\n",
    "    \n",
    "    for (root, dirs, files) in os.walk(path, topdown=True):\n",
    "        for file in files:\n",
    "            filelist.append(os.path.join(root, file))\n",
    "            \n",
    "            if len(filelist) == 50:\n",
    "                return filelist\n",
    "\n",
    "    return filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query.txt', '../testset/sci.space/60777', '../testset/sci.space/61312', '../testset/sci.space/61023', '../testset/sci.space/61090', '../testset/sci.space/61145', '../testset/sci.space/61245', '../testset/sci.space/61042', '../testset/sci.space/61433', '../testset/sci.space/61064', '../testset/sci.space/60978', '../testset/sci.space/60894', '../testset/sci.space/60910', '../testset/sci.space/60785', '../testset/sci.space/60252', '../testset/sci.space/61035', '../testset/sci.space/60888', '../testset/sci.space/61186', '../testset/sci.space/60253', '../testset/sci.space/61138', '../testset/sci.space/62477', '../testset/sci.space/60808', '../testset/sci.space/61157', '../testset/sci.space/61492', '../testset/sci.space/61515', '../testset/sci.space/60845', '../testset/sci.space/60896', '../testset/sci.space/61445', '../testset/sci.space/61036', '../testset/sci.space/61551', '../testset/sci.space/61320', '../testset/sci.space/60916', '../testset/sci.space/60170', '../testset/sci.space/62427', '../testset/sci.space/62317', '../testset/sci.space/61060', '../testset/sci.space/61016', '../testset/sci.space/60934', '../testset/sci.space/60212', '../testset/sci.space/60168', '../testset/sci.space/62393', '../testset/sci.space/60827', '../testset/sci.space/60157', '../testset/sci.space/61274', '../testset/sci.space/61490', '../testset/sci.space/61148', '../testset/sci.space/61221', '../testset/sci.space/61144', '../testset/sci.space/61352', '../testset/sci.space/61198', '../testset/sci.space/60857', 'df']\n"
     ]
    }
   ],
   "source": [
    "doclist = []\n",
    "\n",
    "doclist.append('query.txt')\n",
    "\n",
    "doclist.extend(fileCollector(r'../testset/sci.space'))\n",
    "\n",
    "tfidfDataFrame = mapToMatrix(tfidfMapBuilder(doclist), len(doclist))\n",
    "\n",
    "# tfidfDataFrame\n",
    "# tfidfMapBuilder.to_csv('tfidf.csv')\n",
    "\n",
    "# tfidfMap = tfidfMapBuilder(doclist)\n",
    "\n",
    "# import sqlite3\n",
    "# database = \"../example.sqlite\"\n",
    "# conn = sqlite3.connect(database)\n",
    "# tfidfMap.to_sql(name='Users', con=conn, if_exists='replace', index=False)\n",
    "# conn.close()\n",
    "\n",
    "# print(\"saved to database!!!!!!!!!!!!!!!!\")\n",
    "# \n",
    "# cnx = sqlite3.connect(database)\n",
    "# tfidfMap = pd.read_sql_query(\"SELECT * FROM Users\", cnx)\n",
    "\n",
    "# print(tfidfMap.to_numpy())\n",
    "\n",
    "# tfidfDataFrame = mapToMatrix(tfidfMap, len(doclist))\n",
    "\n",
    "\n",
    "# # tfidfMap.to_sql('tfidf', con=engine, if_exists='replace')\n",
    "\n",
    "\n",
    "doc_tfidfMatrix = tfidfDataFrame.loc[:, tfidfDataFrame.columns != 'query.txt'].to_numpy().round(decimals=4)\n",
    "query_vector = tfidfDataFrame.loc[:, \"query.txt\"].to_numpy().round(decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('../testset/sci.space/60827', 1.4055722894202503)\n",
      "('../testset/sci.space/61138', 1.4614697276429125)\n",
      "('../testset/sci.space/61090', 1.5377657813520584)\n",
      "('../testset/sci.space/60777', 1.5707963267948966)\n",
      "('../testset/sci.space/61312', 1.5707963267948966)\n"
     ]
    }
   ],
   "source": [
    "scorelist = cosineSimilarityScore(query_vector, doc_tfidfMatrix)\n",
    "scoremap = {}\n",
    "\n",
    "for index, score in enumerate(scorelist):\n",
    "    scoremap[tfidfDataFrame.columns[index + 1]] = score\n",
    "\n",
    "scoremap = dict(sorted(scoremap.items(), key=lambda item:item[1]))\n",
    "\n",
    "for doc in list(scoremap.items())[:5]:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query.txt', '../testset/sci.space/60777', '../testset/sci.space/61312', '../testset/sci.space/61023', '../testset/sci.space/61090', '../testset/sci.space/61145', '../testset/sci.space/61245', '../testset/sci.space/61042', '../testset/sci.space/61433', '../testset/sci.space/61064', '../testset/sci.space/60978', '../testset/sci.space/60894', '../testset/sci.space/60910', '../testset/sci.space/60785', '../testset/sci.space/60252', '../testset/sci.space/61035', '../testset/sci.space/60888', '../testset/sci.space/61186', '../testset/sci.space/60253', '../testset/sci.space/61138', '../testset/sci.space/62477', '../testset/sci.space/60808', '../testset/sci.space/61157', '../testset/sci.space/61492', '../testset/sci.space/61515', '../testset/sci.space/60845', '../testset/sci.space/60896', '../testset/sci.space/61445', '../testset/sci.space/61036', '../testset/sci.space/61551', '../testset/sci.space/61320', '../testset/sci.space/60916', '../testset/sci.space/60170', '../testset/sci.space/62427', '../testset/sci.space/62317', '../testset/sci.space/61060', '../testset/sci.space/61016', '../testset/sci.space/60934', '../testset/sci.space/60212', '../testset/sci.space/60168', '../testset/sci.space/62393', '../testset/sci.space/60827', '../testset/sci.space/60157', '../testset/sci.space/61274', '../testset/sci.space/61490', '../testset/sci.space/61148', '../testset/sci.space/61221', '../testset/sci.space/61144', '../testset/sci.space/61352', '../testset/sci.space/61198', '../testset/sci.space/60857']\n"
     ]
    }
   ],
   "source": [
    "doclist = []\n",
    "\n",
    "doclist.append('query.txt')\n",
    "\n",
    "doclist.extend(fileCollector(r'../testset/sci.space'))\n",
    "\n",
    "print(doclist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
